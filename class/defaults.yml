parameters:
  openshift4_monitoring:
    namespace: openshift-monitoring
    # TODO: select based on reported OCP version once we have dynamic facts
    manifests_version: release-4.7
    =_cluster_monitoring_operator_version_map:
      # For release-4.7 we're using the old legacy commit hash
      # No idea what this points to exactly, -SG,2021-10-29.
      release-4.7: 45cb6f2942dff5afe17bbbf46f1eb6d3e7708f51
      release-4.8: release-4.8
      release-4.9: release-4.9
    =_etcd_operator_version_map:
      release-4.7: '' # not required for 4.7 since the cluster-monitoring-operator has the etcd rules
      release-4.8: release-4.9 # etcd-operator Jsonnet only available for 4.9
      release-4.9: release-4.9
    jsonnetfile_parameters:
      cmo_version: ${openshift4_monitoring:_cluster_monitoring_operator_version_map:${openshift4_monitoring:manifests_version}}
      etcd_version: ${openshift4_monitoring:_etcd_operator_version_map:${openshift4_monitoring:manifests_version}}
    defaultConfig:
      nodeSelector:
        node-role.kubernetes.io/infra: ''
    enableUserWorkload: false
    upstreamRules:
      networkPlugin: openshift-sdn
      elasticsearchOperator: true
      clusterSamplesOperator: true
    configs:
      prometheusK8s:
        externalLabels:
          cluster_id: ${cluster:name}
          tenant_id: ${cluster:tenant}
        retention: 8d
        volumeClaimTemplate:
          spec:
            resources:
              requests:
                storage: 50Gi
      prometheusOperator: {}
      alertmanagerMain:
        volumeClaimTemplate:
          spec:
            resources:
              requests:
                storage: 2Gi
      kubeStateMetrics: {}
      grafana: {}
      telemeterClient: {}
      k8sPrometheusAdapter: {}
      openshiftStateMetrics: {}
      thanosQuerier: {}
    configsUserWorkload:
      prometheusOperator: {}
      prometheus: ${openshift4_monitoring:configs:prometheusK8s}
      thanosRuler: {}
    alertManagerConfig:
      route:
        group_wait: 0s
        group_interval: 5s
        repeat_interval: 10m
      inhibit_rules:
        # Don't send warning or info if a critical is already firing
        - target_match_re:
            severity: warning|info
          source_match:
            severity: critical
          equal:
            - namespace
            - alertname
        # Don't send info if a warning is already firing
        - target_match_re:
            severity: info
          source_match:
            severity: warning
          equal:
            - namespace
            - alertname
    alerts:
      ignoreNames: []
      customAnnotations: {}
      patchRules:
        release-4.7:
          SystemMemoryExceedsReservation:
            for: 15m

    silence:
      schedule: '0 */4 * * *'
      serviceAccountName: prometheus-k8s
      servingCertsCABundleName: serving-certs-ca-bundle
      jobHistoryLimit:
        failed: 3
        successful: 3

    rules: {}
    images:
      oc:
        image: quay.io/appuio/oc
        tag: v4.6


    capacityAlerts:
      enabled: false
      groups:
        PodCapacity:
          rules:
            TooManyPods:
              enabled: true
              annotations:
                message: 'Only {{ $value }} more pods can be started.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/podcapacity.html#SYN_TooManyPods
                description: 'The cluster is close to the limit of running pods. The cluster might not be able to handle a node failure and might not be able to start new pods. Consider adding new nodes.'
              for: 30m
              labels: {}
              expr:
                # How many more pods need to be schedulable
                threshold: 'max(kube_node_status_capacity{resource="pods"} * on(node) group_left kube_node_role{role="app"})'
            ExpectTooManyPods:
              enabled: true
              annotations:
                message: 'Expected to exceed the threshold of running pods in 3 days'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/podcapacity.html#SYN_ExpectTooManyPods
                description: 'The cluster is getting close to the limit of running pods. Soon the cluster might not be able to handle a node failure and might not be able to start new pods. Consider adding new nodes.'
              for: 1h
              labels: {}
              expr:
                # How many more pods need to be schedulable in three days
                threshold: 'max(kube_node_status_capacity{resource="pods"} * on(node) group_left kube_node_role{role="app"})'
                # How much of the past to consider for the prediction
                range: '1d'
                # How far into the future to predict (in seconds)
                predict: '3*24*60*60'

        ResourceRequests:
          rules:
            TooMuchMemoryRequested:
              enabled: true
              annotations:
                message: 'Only {{ $value }} memory left for new pods.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_TooMuchMemoryRequested
                description: 'The cluster is close to asigning all memory to running pods. The cluster might not be able to handle a node failure and might not be able to start new pods. Consider adding new nodes.'
              for: 30m
              labels: {}
              expr:
                # How much memory needs to be available to allocate (in bytes)
                threshold: 'max(kube_node_status_capacity{resource="memory"} * on(node) group_left kube_node_role{role="app"})'
            ExpectTooMuchMemoryRequested:
              enabled: true
              annotations:
                message: 'Expected to exceed the threshold of requested memory in 3 days'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_ExpectTooMuchMemoryRequested
                description: 'The cluster is getting close to asigning all memory to running pods. Soon the cluster might not be able to handle a node failure and might not be able to start new pods. Consider adding new nodes.'
              for: 1h
              labels: {}
              expr:
                # How much memory needs to be available to allocate in three days (in bytes)
                threshold: 'max(kube_node_status_capacity{resource="memory"} * on(node) group_left kube_node_role{role="app"})'
                # How much of the past to consider for the prediction
                range: '1d'
                # How far into the future to predict (in seconds)
                predict: '3*24*60*60'
            TooMuchCPURequested:
              enabled: true
              annotations:
                message: 'Only {{ $value }} cpu cores left for new pods.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_TooMuchCPURequested
                description: 'The cluster is close to asigning all CPU resources to running pods. The cluster might not be able to handle a node failure and might soon not be able to start new pods. Consider adding new nodes.'
              for: 30m
              labels: {}
              expr:
                # How many cpu cores need to be available to allocate
                threshold: 'max(kube_node_status_capacity{resource="cpu"} * on(node) group_left kube_node_role{role="app"})'
            ExpectTooMuchCPURequested:
              enabled: true
              annotations:
                message: 'Expected to exceed the threshold of requested CPU resources in 3 days'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_ExpectTooMuchCPURequested
                description: 'The cluster is getting close to asigning all CPU cores to running pods. Soon the cluster might not be able to handle a node failure and might not be able to start new pods. Consider adding new nodes.'
              for: 1h
              labels: {}
              expr:
                # How many cpu cores need to be available to allocate in three days
                threshold: 'max(kube_node_status_capacity{resource="cpu"} * on(node) group_left kube_node_role{role="app"})'
                # How much of the past to consider for the prediction
                range: '1d'
                # How far into the future to predict (in seconds)
                predict: '3*24*60*60'

        MemoryCapacity:
          rules:
            ClusterLowOnMemory:
              enabled: true
              annotations:
                message: 'Only {{ $value }} free memory on Worker Nodes.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/memorycapacity.html#SYN_ClusterMemoryUsageHigh
                description: 'The cluster is close to using all of its memory. The cluster might not be able to handle a node failure or load spikes. Consider adding new nodes.'
              for: 30m
              labels: {}
              expr:
                # How much memory needs to be free over all worker nodes (in bytes)
                threshold: 'max(kube_node_status_capacity{resource="memory"} * on(node) group_left kube_node_role{role="app"})'
            ExpectClusterLowOnMemory:
              enabled: true
              annotations:
                message: 'Cluster expected to run low on memory in 3 days'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/memorycapacity.html#SYN_ExpectClusterMemoryUsageHigh
                description: 'The cluster is getting close to using all of its memory. Soon the cluster might not be able to handle a node failure or load spikes. Consider adding new nodes.'
              for: 1h
              labels: {}
              expr:
                # How much memory needs to be over all worker nodes in three days (in bytes)
                threshold: 'max(kube_node_status_capacity{resource="memory"} * on(node) group_left kube_node_role{role="app"})'
                # How much of the past to consider for the prediction
                range: '1d'
                # How far into the future to predict (in seconds)
                predict: '3*24*60*60'

        CpuCapacity:
          rules:
            ClusterCpuUsageHigh:
              enabled: true
              annotations:
                message: 'Only {{ $value }} idle cpu cores accross cluster.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/cpucapacity.html#SYN_ClusterCpuUsageHigh
                description: 'The cluster is close to using up all CPU resources. The cluster might not be able to handle a node failure or load spikes. Consider adding new nodes.'
              for: 2h
              labels: {}
              expr:
                # How many cpu cores need to be available to allocate
                threshold: 'max(kube_node_status_capacity{resource="cpu"} * on(node) group_left kube_node_role{role="app"})'

            ExpectClusterCpuUsageHigh:
              enabled: true
              annotations:
                message: 'Cluster expected to run low on available CPU resources in 3 days'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/cpucapacity.html#SYN_ExpectClusterCpuUsageHigh
                description: 'The cluster is getting close to using up all CPU resources. The cluster might soon not be able to handle a node failure or load spikes. Consider adding new nodes.'
              for: 2h
              labels: {}
              expr:
                # How many cpu cores need to be idle
                threshold: 'max(kube_node_status_capacity{resource="cpu"} * on(node) group_left kube_node_role{role="app"})'
                # How much of the past to consider for the prediction
                range: '1d'
                # How far into the future to predict (in seconds)
                predict: '3*24*60*60'
