parameters:
  openshift4_monitoring:
    =_metadata:
      library_aliases:
        prom.libsonnet: openshift4-monitoring-prom.libsonnet
    namespace: openshift-monitoring
    # TODO: select based on reported OCP version once we have dynamic facts
    manifests_version: release-4.9
    =_cluster_monitoring_operator_version_map:
      release-4.9: release-4.9
      release-4.10: release-4.10
    =_etcd_operator_version_map:
      release-4.9: release-4.9
      release-4.10: release-4.10
    =_operator_lifecycle_manager_map:
      release-4.9: release-4.9
      release-4.10: release-4.9 # no 4.10 release yet
    jsonnetfile_parameters:
      cmo_version: ${openshift4_monitoring:_cluster_monitoring_operator_version_map:${openshift4_monitoring:manifests_version}}
      etcd_version: ${openshift4_monitoring:_etcd_operator_version_map:${openshift4_monitoring:manifests_version}}
    defaultConfig:
      nodeSelector:
        node-role.kubernetes.io/infra: ''
    enableUserWorkload: false
    upstreamRules:
      networkPlugin: openshift-sdn
      elasticsearchOperator: true
      clusterSamplesOperator: true
    configs:
      prometheusK8s:
        externalLabels:
          cluster_id: ${cluster:name}
          tenant_id: ${cluster:tenant}
        retention: 8d
        volumeClaimTemplate:
          spec:
            resources:
              requests:
                storage: 50Gi
      prometheusOperator: {}
      alertmanagerMain:
        volumeClaimTemplate:
          spec:
            resources:
              requests:
                storage: 2Gi
      kubeStateMetrics: {}
      grafana: {}
      telemeterClient: {}
      k8sPrometheusAdapter: {}
      openshiftStateMetrics: {}
      thanosQuerier: {}
    configsUserWorkload:
      prometheusOperator: {}
      prometheus: ${openshift4_monitoring:configs:prometheusK8s}
      thanosRuler: {}
    alertManagerConfig:
      route:
        group_wait: 0s
        group_interval: 5s
        repeat_interval: 10m
      inhibit_rules:
        # Don't send warning or info if a critical is already firing
        - target_match_re:
            severity: warning|info
          source_match:
            severity: critical
          equal:
            - namespace
            - alertname
        # Don't send info if a warning is already firing
        - target_match_re:
            severity: info
          source_match:
            severity: warning
          equal:
            - namespace
            - alertname
    alerts:
      ignoreNames: []
      customAnnotations: {}
      patchRules: {}

    silence:
      schedule: '0 */4 * * *'
      serviceAccountName: prometheus-k8s
      servingCertsCABundleName: serving-certs-ca-bundle
      jobHistoryLimit:
        failed: 3
        successful: 3
      nodeSelector:
        node-role.kubernetes.io/infra: ''

    rules: {}
    images:
      oc:
        image: quay.io/appuio/oc
        tag: v4.9


    capacityAlerts:
      enabled: false
      groups:
        PodCapacity:
          rules:
            TooManyPods:
              enabled: true
              annotations:
                message: 'Only {{ $value }} more pods can be started.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/podcapacity.html#SYN_TooManyPods
                description: 'The cluster is close to the limit of running pods. The cluster might not be able to handle node failures and might not be able to start new pods. Consider adding new nodes.'
              for: 30m
              labels: {}
              expr:
                # How many more pods need to be schedulable
                threshold: 'max(kube_node_status_capacity{resource="pods"} * on(node) group_left kube_node_role{role="app"})'
                # Multiply the threshold by this factor
                factor: 1
            ExpectTooManyPods:
              enabled: false
              annotations:
                message: 'Expected to exceed the threshold of running pods in 3 days'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/podcapacity.html#SYN_ExpectTooManyPods
                description: 'The cluster is getting close to the limit of running pods. Soon the cluster might not be able to handle node failures and might not be able to start new pods. Consider adding new nodes.'
              for: 3h
              labels: {}
              expr:
                # How many more pods need to be schedulable in three days
                threshold: 'max(kube_node_status_capacity{resource="pods"} * on(node) group_left kube_node_role{role="app"})'
                # Multiply the threshold by this factor
                factor: 1
                # How much of the past to consider for the prediction
                range: '1d'
                # How far into the future to predict (in seconds)
                predict: '3*24*60*60'

        ResourceRequests:
          rules:
            TooMuchMemoryRequested:
              enabled: true
              annotations:
                message: 'Only {{ $value }} memory left for new pods.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_TooMuchMemoryRequested
                description: 'The cluster is close to asigning all memory to running pods. The cluster might not be able to handle node failures and might not be able to start new pods. Consider adding new nodes.'
              for: 30m
              labels: {}
              expr:
                # How much memory needs to be available to allocate (in bytes)
                threshold: 'max(kube_node_status_allocatable{resource="memory"} * on(node) group_left kube_node_role{role="app"})'
                # Multiply the threshold by this factor
                factor: 1
            ExpectTooMuchMemoryRequested:
              enabled: false
              annotations:
                message: 'Expected to exceed the threshold of requested memory in 3 days'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_ExpectTooMuchMemoryRequested
                description: 'The cluster is getting close to asigning all memory to running pods. Soon the cluster might not be able to handle node failures and might not be able to start new pods. Consider adding new nodes.'
              for: 3h
              labels: {}
              expr:
                # How much memory needs to be available to allocate in three days (in bytes)
                threshold: 'max(kube_node_status_allocatable{resource="memory"} * on(node) group_left kube_node_role{role="app"})'
                # Multiply the threshold by this factor
                factor: 1
                # How much of the past to consider for the prediction
                range: '1d'
                # How far into the future to predict (in seconds)
                predict: '3*24*60*60'
            TooMuchCPURequested:
              enabled: true
              annotations:
                message: 'Only {{ $value }} cpu cores left for new pods.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_TooMuchCPURequested
                description: 'The cluster is close to asigning all CPU resources to running pods. The cluster might not be able to handle node failures and might soon not be able to start new pods. Consider adding new nodes.'
              for: 30m
              labels: {}
              expr:
                # How many cpu cores need to be available to allocate
                threshold: 'max(kube_node_status_allocatable{resource="cpu"} * on(node) group_left kube_node_role{role="app"})'
                # Multiply the threshold by this factor
                factor: 1
            ExpectTooMuchCPURequested:
              enabled: false
              annotations:
                message: 'Expected to exceed the threshold of requested CPU resources in 3 days'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_ExpectTooMuchCPURequested
                description: 'The cluster is getting close to asigning all CPU cores to running pods. Soon the cluster might not be able to handle node failures and might not be able to start new pods. Consider adding new nodes.'
              for: 3h
              labels: {}
              expr:
                # How many cpu cores need to be available to allocate in three days
                threshold: 'max(kube_node_status_allocatable{resource="cpu"} * on(node) group_left kube_node_role{role="app"})'
                # Multiply the threshold by this factor
                factor: 1
                # How much of the past to consider for the prediction
                range: '1d'
                # How far into the future to predict (in seconds)
                predict: '3*24*60*60'

        MemoryCapacity:
          rules:
            ClusterLowOnMemory:
              enabled: true
              annotations:
                message: 'Only {{ $value }} free memory on Worker Nodes.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/memorycapacity.html#SYN_ClusterMemoryUsageHigh
                description: 'The cluster is close to using all of its memory. The cluster might not be able to handle node failures or load spikes. Consider adding new nodes.'
              for: 30m
              labels: {}
              expr:
                # How much memory needs to be free over all worker nodes (in bytes)
                threshold: 'max(kube_node_status_capacity{resource="memory"} * on(node) group_left kube_node_role{role="app"})'
                # Multiply the threshold by this factor
                factor: 1
            ExpectClusterLowOnMemory:
              enabled: false
              annotations:
                message: 'Cluster expected to run low on memory in 3 days'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/memorycapacity.html#SYN_ExpectClusterMemoryUsageHigh
                description: 'The cluster is getting close to using all of its memory. Soon the cluster might not be able to handle node failures or load spikes. Consider adding new nodes.'
              for: 3h
              labels: {}
              expr:
                # How much memory needs to be over all worker nodes in three days (in bytes)
                threshold: 'max(kube_node_status_capacity{resource="memory"} * on(node) group_left kube_node_role{role="app"})'
                # Multiply the threshold by this factor
                factor: 1
                # How much of the past to consider for the prediction
                range: '1d'
                # How far into the future to predict (in seconds)
                predict: '3*24*60*60'

        CpuCapacity:
          rules:
            ClusterCpuUsageHigh:
              enabled: true
              annotations:
                message: 'Only {{ $value }} idle cpu cores accross cluster.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/cpucapacity.html#SYN_ClusterCpuUsageHigh
                description: 'The cluster is close to using up all CPU resources. The cluster might not be able to handle node failures or load spikes. Consider adding new nodes.'
              for: 30m
              labels: {}
              expr:
                # How many cpu cores need to be available to allocate
                threshold: 'max(kube_node_status_capacity{resource="cpu"} * on(node) group_left kube_node_role{role="app"})'
                # Multiply the threshold by this factor
                factor: 1

            ExpectClusterCpuUsageHigh:
              enabled: false
              annotations:
                message: 'Cluster expected to run low on available CPU resources in 3 days'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/cpucapacity.html#SYN_ExpectClusterCpuUsageHigh
                description: 'The cluster is getting close to using up all CPU resources. The cluster might soon not be able to handle node failures or load spikes. Consider adding new nodes.'
              for: 3h
              labels: {}
              expr:
                # How many cpu cores need to be idle
                threshold: 'max(kube_node_status_capacity{resource="cpu"} * on(node) group_left kube_node_role{role="app"})'
                # Multiply the threshold by this factor
                factor: 1
                # How much of the past to consider for the prediction
                range: '1d'
                # How far into the future to predict (in seconds)
                predict: '3*24*60*60'

        UnusedCapacity:
          rules:
            ClusterHasUnusedNodes:
              enabled: true
              annotations:
                message: 'Cluster has unused nodes.'
                runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/unusedcapacity.html#SYN_ClusterHasUnusedNodes
                description: 'The cluster has {{ $value }} unused nodes. Consider removing unused nodes.'
              for: 8h
              labels: {}
              expr:
                # How many nodes need to be unused.
                # There should be some overcapacity to account for failing nodes and future growth.
                reserved: 4
